#GCN

无穷的远方，无尽的人们，都和我有关
今天研究了一下Graph Convolutional Networks.
link:
https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=2ahUKEwiFweetzfzfAhVJHDQIHYhlB-gQFjABegQIBxAC&url=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DSJU4ayYgl&usg=AOvVaw1Ca-hHcv-H9Tp-erfaB-RL
对于没有CS专业背景的大多数社会科学研究人员来说，理解一个深度学习模型着实是一个难度不小的工程。本文将尽可能的避免专业术语所造成的壁垒以及过于tricky的数学过程，将GCN的基本原理做一个介绍。
###什么是GCN
一句话概括，GCN的精髓就是通过网络中临近节点的关系来预测节点的属性，简言之是一个解决分类问题的模型。
首先，GCN依然可以被理解为一种神经网络模型，包含了输入层，隐藏层和输出层。和普通NN不同的是，GCN的输入层并不是普通的one-hot编码，而是包含节点网络信息以及节点属性的复合编码。在隐藏层中，GCN不是单单的将编码投射在高维空间中，而是在投射过程中通过卷积核的扫描将相临近的节点对目标节点的影响同时传入隐藏层。通过多次迭代，以达到输入“远方”节点影响力进行训练的效果。

###输入层
在GCN里，我们首先需要输入一个包含了n维的邻接矩阵（adjacency matrix）。这个邻接矩阵需要通过预先的数据处理获得。矩阵代表了研究对象的网络信息，通常可以是文档网络，也可以是人际关系网络等等。需要注意的是，这个邻接矩阵每个点之间的关系存在着一个固定的权重。这种权重使得邻接矩阵变成了一个转移概率矩阵。转移概率矩阵的意义在于每变换可以传导相邻节点对于目标节点的影响。需要特别注意的是，GCN中这个影响的权重是固定不变的。
除了n维的adj matrix,我们还需要输入每个节点的label。举个例子，假如这个网络是人际关系网络的话，我们输入的label就可能会是每个节点（每个人）的教育、收入、性别等信息。令人叹为观止的是，这个label信息可以包含大量缺失值。而GCN所能够做的，就是根据这个人际关系网络倒推出这些缺失值，而且准确率还不低。比如在文章所使用的代码中，训练数据仅占不到20%，而准确率依然可以达到80%。

在输入了邻接矩阵和label信息后，我们还需要输入每一个节点的feature信息。如果每个节点代表一篇文章，那么这个feature信息就可以是一个bag of words，即包含了关键词信息的词袋。这个时候我们会发现我们已经输入了我们所需要的全部信息。即节点本身（feature）、网络关系（adj matrix）以及标签信息(label)。

###隐藏层
准备好数据后，我们来到了最为关键的GC层。吴令飞博士认为GC变换的insight在于一个embedding过程和voting过程。首先我们将含有特征信息并且长度为n的网络矩阵投射到一个m维平面上。在提取特征后，我们将这个新的矩阵和准备好的转移概率矩阵点乘，这一步的意义在于将相邻节点的影响信息传入embedding。这样一来，一次GC变换过后我们将得到一个n*m的矩阵。这是一个被提取了特征的，包含了每个节点相邻接点影响信息的矩阵。此时我们需要将这个新的矩阵再做一次GC变换，这次变换的意义主要在于将更远处的节点的影响传入矩阵。
######GC1--->GC2--->…GCn
不过通常情况下，三层网络即可达到比较理想的效果了。
到这一步我们可以发现，每一次GC变换除了提取特征之外最大的意义就是在于不断传入附近节点的影响。由于每个节点在GC变换过程中都记录了周边节点的信息，因此多次GC变换过后整个网络的关系数据都被传入了计算。

###输出层
不管中间进行了多少次变换，最后一次GC变换的m都需要与我们的label数量相等。这样我们的神经网络才算定义完整。
如今我们就知道从输入，隐藏到输出层的完整运作流程了。

###如何应用
在明确了GCN的运作原理后，我们知道了GCN可以通过网络关系推测每一个节点的标签。这样一来就会产生很有趣的应用场景。比方说在研究社交媒体数据时，通过定义好的社交网络数据推测每个用户的性别、学历以及其他有价值的标签数据。此外，在问卷调查中，如果我们的问题可以用合适的网络进行表示，那么GCN就可以被用来推测每一道问题的缺失值了。




提纲：
1 GCN简介
2 神经网络背景介绍
3 GCN流程介绍

